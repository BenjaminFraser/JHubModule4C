{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JHub Coding Module 4C Guidance and Submission Template\n",
    "\n",
    "This notebook is a template for module 4C from the JHub Coding Scheme. \n",
    "\n",
    "**Important:** Please do not change or modify the code within Sections 1-3 of this notebook, with exception to Section 2.2 for your chosen dependencies. The prepopulated code has been provided to save difficulties with downloading and extracting the dataset for this challenge.\n",
    "\n",
    "When you make a submission, your notebook will be tested in Google Colab, so it is essential that your code has been tested and works without any issues on the Google Colab platform.\n",
    "\n",
    "This notebook has been produced to improve the consistency of submissions, and to act as a basic starting point for the challenge. Despite this, the challenge still aims to give students a good amount of flexibility in applying a range of techniques and chosen processes to solve the challenge. This challenge is designed to be a considerable step-up compared to Challenge 4B, and as such should test the ability of students more thoroughly for applying various principles of data science and machine learning.\n",
    "\n",
    "You need to populate the required functions to solve this problem. All dependencies should be documented in the next cell.\n",
    "\n",
    "**You can:** add further cells or text blocks to extend or further explain your solution add further functions\n",
    "\n",
    "**Dont:** rename the helper functions and classes within the notebook, since this ensures consistency during testing of submissions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Challenge Overview and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Requirements:**\n",
    "\n",
    "In summary, this challenge consists of the following tasks:\n",
    "\n",
    "1. Loading and processing of a large image dataset, which is stored in a realistic directory structure.\n",
    "\n",
    "2. Preprocessing and preparation of image data and labels into a suitable form for a DNN.\n",
    "\n",
    "3. Model production, tuning and evaluation of performance on the image classification task using the training and test sets provided.\n",
    "\n",
    "4. Final model predictions on a seperately held-out test set (supplied with no labels), and saving the prediction labels as .csv. A helper function is provided at the bottom of this template for loading this final data (labels are not provided).\n",
    "\n",
    "---\n",
    "\n",
    "**Introducing the dataset and model requirements:**\n",
    "\n",
    "For this challenge you must produce a functioning Deep Neural Network model that classifies a 32x32 RGB images into one of three possible classes: Aircraft, Ships or Automobiles. \n",
    "\n",
    "The dataset provided consists of two splits of data - the training and test splits. Each of these have been split using stratified sampling techniques, with an equal proportion of data available for each class, and so imbalanced data is not an issue in this challenge. \n",
    "\n",
    "Within each split, there are a large number of .png images for each class. These are organised into a directory structure, like so:\n",
    "\n",
    "    - Train\n",
    "        - aircraft \n",
    "            [1000 PNG formatted 32x32 RBG images]\n",
    "        - automobile\n",
    "            [1000 PNG formatted 32x32 RBG images]\n",
    "        - ship\n",
    "            [1000 PNG formatted 32x32 RBG images]\n",
    "        \n",
    "    - Test\n",
    "        - aircraft\n",
    "            [333 PNG formatted 32x32 RBG images]\n",
    "        - automobile\n",
    "            [333 PNG formatted 32x32 RBG images]\n",
    "        - ship\n",
    "            [333 PNG formatted 32x32 RBG images]\n",
    "            \n",
    "\n",
    "To complete the task, you must first load and preprocess the images into suitable training and test datasets, followed by the training, tuning and evaluation of a Deep Neural Network model. The aim is to produce a tuned model that generalises as highly to the test set as possible. \n",
    "\n",
    "As a rule of thumb, if you are achieving an accuracy of 75% or higher on the test set (having only trained on training data), your model is performing well.\n",
    "\n",
    "You are not limited to a particular type of Deep Learning Model, and may apply any architecture type you like. However, for the purpose of this challenge you should use the TensorFlow and the Keras frameworks for model production.\n",
    "\n",
    "---\n",
    "**Final predictions on held-out dataset:**\n",
    "\n",
    "Once your model is finalised and you have evaluated the performance on the test dataset, you must make final predictions on a held-out private test set. This private test set can be loaded using the function provided at the bottom of this notebook. Please note - no labels are provided for this data, and it is up to you to provide labels for this data by making predictions with your model. The final prediction labels for these must be provided as a .csv with a label per row for each image. **Hint:** It's highly recommended to format your predictions as a Pandas series, followed by simply saving this as a .csv using .to_csv().\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fixed Dependencies - Leave these as they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed dependencies - do not remove or change.\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Custom dependencies - add whatever you need into here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your dependencies here, e.g. tensorflow, keras, matplotlib..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Downloading and extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note** - Only do one of the following methods for obtaining the dataset. You don't need to run all three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Downloading and extracting using curl and unzip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands will download the dataset, as a .zip file, and then extract it into the local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the following command sufficient time to download the 10 Mb dataset before running the next cell\n",
    "!curl -L \"https://github.com/BenjaminFraser/JHubModule4C/blob/main/Module_4C_Dataset.zip?raw=true\" > dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove zip file - no longer needed\n",
    "!rm dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Alternative method - downloading and extracting using wget (only if previous method did not work):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if the curl command above does not work on your OS, try wget instead, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://github.com/BenjaminFraser/JHubModule4C/blob/main/Module_4C_Dataset.zip?raw=true\" -O dataset.zip\n",
    "!unzip dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip dataset.zip\n",
    "!rm dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Manual method (if two methods above both fail) - download .zip file from Github and extract manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, if niether of these methods not on your OS, simply navigate to the following url:\n",
    "\n",
    "- https://github.com/BenjaminFraser/JHubModule4C/blob/main/Module_4C_Dataset.zip?raw=true\n",
    "\n",
    "Download the zip file manually, and unzip into the local directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have the entire dataset within your local directory, named 'Data'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading and processing the image data and labels into a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Data/train\"\n",
    "test_dir = \"Data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(data_dir, img_height=32, img_width=32, channels=3):\n",
    "    \"\"\"This function needs to import and preprocess the image data appropriately. \n",
    "       The techniques for preprocessing and handling the data are not set, and \n",
    "       you can apply your own methodology as you choose.\n",
    "       \n",
    "       It's recommended that you create a function that loads and processes\n",
    "       the image data based on a chosen directory given, e.g. train data filepath, \n",
    "       which then returns the associated image data as np arrays, and labels as a \n",
    "       pandas df or series as appropriate. \"\"\"\n",
    "        \n",
    "    return image_dataset, image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and test images into memory\n",
    "train_images, train_labels = load_and_process_data(train_dir)\n",
    "test_images, test_labels = load_and_process_data(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing and preparation of the Image Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module4C:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "       \n",
    "    def preprocess_image_data(self, image_data):\n",
    "        \"\"\" This function should process your image data prior to training as \n",
    "            appropriate, along with storing any required features in the class \"\"\"\n",
    "        \n",
    "        return preprocessed_images\n",
    "\n",
    "    \n",
    "    def preprocess_labels(self, img_labels):\n",
    "        \"\"\" Preprocess image labels (if required), storing any features \n",
    "            required in the class \"\"\"\n",
    "            \n",
    "        return final_labels\n",
    "\n",
    "    \n",
    "    def decode_predictions(self, predictions):\n",
    "        \"\"\" Helper function for Decoding predition probabilities from DNN model, \n",
    "            returning as hard output labels \"\"\"  \n",
    "            \n",
    "        return decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_mod4c = Module4C()\n",
    "\n",
    "# preprocess training images and labels\n",
    "X_train_full = tester_mod4c.preprocess_image_data(train_images)\n",
    "y_train_full = tester_mod4c.preprocess_labels(train_labels)\n",
    "\n",
    "# preprocess test images and labels\n",
    "X_test = tester_mod4c.preprocess_image_data(test_images)\n",
    "y_test = tester_mod4c.preprocess_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Production, Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform chosen methodologies for model production, tuning and evaluation of a DNN model.\n",
    "\n",
    "## This is the main section of the assignment and as such will likely involve the most code. \n",
    "\n",
    "## No helper functions or classes have been provided in this section, which is to encourage you\n",
    "## to tackle the problem as you see fit. \n",
    "\n",
    "## Tips / Considerations: \n",
    "##     - Visualisation model performance is very important for DNNs, and makes optimisation much easier\n",
    "##     - Consider the best evaluation metrics to use for this classification problem, is accuracy best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model predictions on hold-out test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criteria:** You must ensure the resultant predictions on the hold-out test set are produced with the labels in string format, e.g. 'aircraft', 'ship' or 'automobile'. Do not submit the predictions in their encoded format, i.e. 0, 1 or 2. \n",
    "\n",
    "The helper function below will download and load the hold-out test images into a numpy array from a remote repository. You must use your existing trained model to make predictions on these images, and produce a set of .csv labels for the 550 images accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_holdout_test_set():\n",
    "    \"\"\" Download the hold-out test set (labels not provided), which you must\n",
    "        use with your trained model to make predictions \"\"\"\n",
    "    \n",
    "    # url to github repo - if this doesn't work, change to kaggle alt url in module guidance\n",
    "    url = 'https://github.com/BenjaminFraser/JHubModule4C/blob/main/holdout_test_images.npy?raw=true'\n",
    "    \n",
    "    try:\n",
    "        request = requests.get(url, allow_redirects=True)\n",
    "        open('holdout_test.npy', 'wb').write(request.content)\n",
    "        test_data = np.load('holdout_test.npy')\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "        return(f\"An error occurred downloading and saving the file: {e}\")\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the holdout test set and load as numpy array\n",
    "hold_out_data = download_holdout_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess hold-out test data and make predictions etc..\n",
    "# format labels as .csv, with labels in string form, e.g.: 'aircraft', 'ship', 'aircraft' ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
