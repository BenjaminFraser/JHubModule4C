{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Module4C-Challenge_Deep_Learning_Template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XdUq3NsanOh"
      },
      "source": [
        "# **JHub Coding Module 4C Guidance and Submission Template**\n",
        "\n",
        "#### **Link to Github Hosted Repo:** https://github.com/BenjaminFraser/JHubModule4C\n",
        "\n",
        "This notebook is a template for module 4C from the JHub Coding Scheme. It has been produced to improve the consistency of submissions, and to act as a basic starting point for the challenge. Despite this, the challenge still aims to give students a good amount of flexibility in applying a range of techniques and chosen processes to solve the challenge. This challenge is designed to be a considerable step-up compared to Challenge 4B, and as such should test the ability of students more thoroughly for applying various principles of data science and machine learning.\n",
        "\n",
        "**Important:** Please do not change or modify the code within Sections 1-3 of this notebook, with exception to Section 2.2 for your chosen dependencies. The prepopulated code has been provided to save difficulties with downloading and extracting the dataset for this challenge. When you make a submission, your notebook will be tested in Google Colab, so it is essential that your code has been tested and works without any issues on the Google Colab platform.\n",
        "\n",
        "You need to populate the required functions to solve this problem. All dependencies should be documented in the next cell.\n",
        "\n",
        "**You can:** add further cells or text blocks to extend or further explain your solution add further functions\n",
        "\n",
        "**Dont:** rename the helper functions and classes within the notebook, since this ensures consistency during testing of submissions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3oeoRieanOh"
      },
      "source": [
        "## **1. Challenge Overview and Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-R9JaWjanOh"
      },
      "source": [
        "### **1.1 Summary of Requirements:**\n",
        "\n",
        "In summary, this challenge consists of the following tasks:\n",
        "\n",
        "1. Loading and processing of a large image dataset, which is stored in a realistic directory structure.\n",
        "\n",
        "2. Preprocessing and preparation of image data and labels into a suitable form for a DNN.\n",
        "\n",
        "3. Model production, tuning and evaluation of performance on the image classification task using the training and test sets provided.\n",
        "\n",
        "4. Your submission must provide a function which can return a prediction when provided with a single image in the same format and dimensions as the training data.  This will be provided in as a png and should return a class prediction as text.\n",
        "\n",
        "5.  Your submission must run on a Google Colab notebook and not take an undue amount of time to train (max 30 mins)  This should be easily achievable under 10 mins.\n",
        "\n",
        "6. Your submission will be tested against an unseen holdout test dataset.\n",
        "\n",
        "7. Your model sh\n",
        "\n",
        "---\n",
        "\n",
        "### **1.2 Introducing the dataset and model requirements:**\n",
        "\n",
        "For this challenge you must produce a functioning Deep Neural Network model that classifies a 32x32 RGB images into one of three possible classes: Aircraft, Ships or Automobiles. \n",
        "\n",
        "The dataset provided consists of two splits of data - the training and test splits. Each of these have been split using stratified sampling techniques, with an equal proportion of data available for each class, and so imbalanced data is not an issue in this challenge. \n",
        "\n",
        "Within each split, there are a large number of .png images for each class. These are organised into a directory structure, like so:\n",
        "\n",
        "    - Train\n",
        "        - aircraft \n",
        "            [1000 PNG formatted 32x32 RBG images]\n",
        "        - automobile\n",
        "            [1000 PNG formatted 32x32 RBG images]\n",
        "        - ship\n",
        "            [1000 PNG formatted 32x32 RBG images]\n",
        "        \n",
        "    - Test\n",
        "        - aircraft\n",
        "            [333 PNG formatted 32x32 RBG images]\n",
        "        - automobile\n",
        "            [333 PNG formatted 32x32 RBG images]\n",
        "        - ship\n",
        "            [333 PNG formatted 32x32 RBG images]\n",
        "            \n",
        "\n",
        "To complete the task, you must first load and preprocess the images into suitable training and test datasets, followed by the training, tuning and evaluation of a Deep Neural Network model. The aim is to produce a tuned model that generalises as highly to the test set as possible. \n",
        "\n",
        "As a rule of thumb, if you are achieving an accuracy of 75% or higher on the test set (having only trained on training data), your model is performing well.\n",
        "\n",
        "You are not limited to a particular type of Deep Learning Model, and may apply any architecture type you like.\n",
        "\n",
        "---\n",
        "### **1.3 Final predictions on held-out dataset:**\n",
        "\n",
        "Once your model is finalised and you have evaluated the performance on the test dataset by the assessor.  Ensure that your code can be run on a Google Colab notebook within time constraints.  Your submission must train a model, which can then make predictions on unseen data of the same form as the training data.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_CjR9kianOh"
      },
      "source": [
        "## **2. Import Dependencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3REAAnnwanOh"
      },
      "source": [
        "### **2.1 Fixed Dependencies - Leave these as they are**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaLwZE_HanOh"
      },
      "source": [
        "# Fixed dependencies - do not remove or change.\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1BB7EUHanOh"
      },
      "source": [
        "### **2.2 Custom dependencies - add whatever you need into here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLpai4PUanOh"
      },
      "source": [
        "# import your dependencies here, e.g. tensorflow, keras, matplotlib..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6KBQzEmanOi"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18P2swk6anOi"
      },
      "source": [
        "## **3. Downloading and extracting the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCM09XYSanOi"
      },
      "source": [
        "**Important note** - Only do one of the following methods for obtaining the dataset. You don't need to run all three."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra6Vs08KanOi"
      },
      "source": [
        "### **3.1 Downloading and extracting using curl and unzip:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQpzvlM6anOi"
      },
      "source": [
        "The following commands will download the dataset, as a .zip file, and then extract it into the local directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEQk3RwZanOi",
        "outputId": "98bcec40-f194-419a-9d78-e2347d7e4306"
      },
      "source": [
        "# give the following command sufficient time to download the 10 Mb dataset before running the next cell\n",
        "!curl -L \"https://github.com/BenjaminFraser/JHubModule4C/blob/main/Module_4C_Dataset.zip?raw=true\" > dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   143  100   143    0     0    725      0 --:--:-- --:--:-- --:--:--   725\n",
            "100   154  100   154    0     0    158      0 --:--:-- --:--:-- --:--:--  150k\n",
            "100 9432k  100 9432k    0     0  5954k      0  0:00:01  0:00:01 --:--:-- 5954k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYvA8hXianOi"
      },
      "source": [
        "!unzip dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJJWmQGmanOi"
      },
      "source": [
        "# remove zip file - no longer needed\n",
        "!rm dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2binzXhBbAHV"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2WZAk-kanOi"
      },
      "source": [
        "### **3.2 Alternative method - downloading and extracting using wget (only if previous method did not work):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU5wxqC5anOi"
      },
      "source": [
        "Alternatively, if the curl command above does not work on your OS, try wget instead, like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdKZgQvqanOi"
      },
      "source": [
        "!wget \"https://github.com/BenjaminFraser/JHubModule4C/blob/main/Module_4C_Dataset.zip?raw=true\" -O dataset.zip\n",
        "!unzip dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAFwjfp7anOi"
      },
      "source": [
        "!unzip dataset.zip\n",
        "!rm dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi_k0s_TanOi"
      },
      "source": [
        "### **3.3 Manual method (if two methods above both fail) - download .zip file from Github and extract manually**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoZrCxN-anOi"
      },
      "source": [
        "Ultimately, if niether of these methods not on your OS, simply navigate to the following url:\n",
        "\n",
        "- https://github.com/BenjaminFraser/JHubModule4C/blob/main/Module_4C_Dataset.zip?raw=true\n",
        "\n",
        "Download the zip file manually, and unzip into the local directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBEzRVuYanOi"
      },
      "source": [
        "You should now have the entire dataset within your local directory, named 'Data'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv6FP52canOi"
      },
      "source": [
        "## **4. Loading and processing the image data and labels into a dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqdeXgEpanOi"
      },
      "source": [
        "train_dir = \"Data/train\"\n",
        "test_dir = \"Data/test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R2vm_bOquSi"
      },
      "source": [
        "You need to import the dataset downloaded into a form suitable for model development, which the helper function below is intended to guide you with.\n",
        "\n",
        "This function needs to import and process the image data appropriately. \n",
        "\n",
        "The techniques for preprocessing and handling the data are not set, and you can apply your own methodology as you choose.\n",
        "\n",
        "The images are currenly stored in the directory structure defined above. Your methodology should involve importing all images and creating a suitable image dataset and corresponding set of class labels.\n",
        "    \n",
        "It's recommended that you create a function that loads and processes the image data based on a chosen directory given, e.g. train data filepath, which then returns the associated image data as np arrays, and labels as a pandas df or series as appropriate. A template function for this is provided below.\n",
        "\n",
        "The suggested output format for the image dataset is a numpy array, with the shape: (dataset_size, 32, 32, 3). \n",
        "\n",
        "The labels can be either a numpy array with the corresponding class ids, or alternatively a pandas series with the string output labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELkKH8n8anOi"
      },
      "source": [
        "def load_and_process_data(data_dir, img_height=32, img_width=32, channels=3):\n",
        "    \"\"\" Example function for loading and processing images stored in \n",
        "        in a directory structure into an input dataset and corresponding labels\n",
        "    \"\"\"\n",
        "        \n",
        "    return image_dataset, image_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3ePQXvianOi"
      },
      "source": [
        "# load training and test images into memory\n",
        "train_images, train_labels = load_and_process_data(train_dir)\n",
        "test_images, test_labels = load_and_process_data(test_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W759OBRanOi"
      },
      "source": [
        "## **5. Preprocessing and preparation of the Image Data and Labels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lnDlWpuanOi"
      },
      "source": [
        "## **6. Model Production, Tuning and Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRQpIsQaanOi"
      },
      "source": [
        "## Perform chosen methodologies for model production, tuning and evaluation of a DNN model.\n",
        "\n",
        "## This is the main section of the assignment and as such will likely involve the most code. \n",
        "\n",
        "## No helper functions or classes have been provided in this section, which is to encourage you\n",
        "## to tackle the problem as you see fit. \n",
        "\n",
        "## Tips / Considerations: \n",
        "##     - Visualisation model performance is very important for DNNs, and makes optimisation much easier\n",
        "##     - Consider the best evaluation metrics to use for this classification problem, is accuracy best?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhd40yKHanOi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NzZBX2BanOi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUDHagIUanOi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHPoWFqSanOi"
      },
      "source": [
        "def predict_class(model, single_image):\n",
        "    \"\"\"\n",
        "    This function must take your trained model and a single image so that we can test your model.\n",
        "    :param model : trained model object\n",
        "    :param single_image: single image file, which will be a .png file with the same dimensions as the test image\n",
        "    \n",
        "    :return: image_class\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAAGhEODanOi"
      },
      "source": [
        "## **7. Model predictions on hold-out test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C-LyCgdanOi"
      },
      "source": [
        "**Criteria:** You must ensure the resultant predictions on the hold-out test set are produced with the labels in string format, e.g. 'aircraft', 'ship' or 'automobile'. Do not submit the predictions in their encoded format, i.e. 0, 1 or 2."
      ]
    }
  ]
}
